# DSX Data Scientist Coding Assessment

Solutions to the DSX coding assessment covering R package development, the Pharmaverse ecosystem (SDTM, ADaM, TLGs), and Python (FastAPI, LangChain).

## Repository Structure

```
dsx/
├── question_1/                  # R Package Development
│   └── descriptive_stats/       # {descriptiveStats} R package
├── question_2_sdtm/             # SDTM DS Domain (sdtm.oak)
├── question_3_adam/              # ADaM ADSL Dataset (admiral)
├── question_4_tlg/              # Clinical Reporting (gtsummary, ggplot2, gt)
├── question_5_api/              # Python FastAPI Application
├── question_6_genai/            # GenAI Clinical Data Assistant (LangChain)
├── QUESTION_[1-6].md            # Question specifications
├── REQUIREMENTS.md              # Assessment requirements
└── KNOWLEDGE_BASE.md            # Reference material
```

## Test Summary

| Question | Tests | Framework |
|----------|-------|-----------|
| Q1 — R Package | 67 | testthat (edition 3) |
| Q2 — SDTM | 49 | testthat |
| Q3 — ADaM | 75 | testthat |
| Q4 — TLGs | 50 | testthat |
| Q5 — API | 39 | pytest |
| Q6 — GenAI | 70 | pytest |
| **Total** | **350** | |

---

## Question 1: R Package — `descriptiveStats`

**Objective:** Build a structured R package implementing descriptive statistics functions.

**Location:** `question_1/descriptive_stats/`

### Package Structure

```
descriptive_stats/
├── DESCRIPTION              # Package metadata (v1.0.0)
├── LICENSE                  # MIT license
├── NAMESPACE                # Generated by roxygen2
├── R/
│   ├── calc_mean.R          # Arithmetic mean
│   ├── calc_median.R        # Median (handles odd/even length)
│   ├── calc_mode.R          # Mode (supports multimodal, returns NA with message if uniform)
│   ├── calc_quartiles.R     # Q1 (25th percentile) and Q3 (75th percentile)
│   ├── calc_iqr.R           # Interquartile range (Q3 - Q1)
│   ├── utils.R              # validate_numeric_input() — shared input validation
│   └── descriptiveStats-package.R  # Package-level docs and imports
├── man/                     # Auto-generated roxygen2 documentation
├── tests/
│   ├── testthat.R
│   └── testthat/
│       ├── test-calc_mean.R      # 15 tests
│       ├── test-calc_median.R    # 12 tests
│       ├── test-calc_mode.R      # 15 tests
│       ├── test-calc_quartiles.R # 16 tests (Q1 + Q3)
│       └── test-calc_iqr.R      # 11 tests
└── README.md
```

### Exported Functions

| Function | Description |
|----------|-------------|
| `calc_mean(x)` | Arithmetic mean, NA-safe |
| `calc_median(x)` | Median, averages two middle values for even-length |
| `calc_mode(x)` | Mode(s), returns sorted vector for multimodal data |
| `calc_q1(x)` | 25th percentile (R type 7 algorithm) |
| `calc_q3(x)` | 75th percentile (R type 7 algorithm) |
| `calc_iqr(x)` | Interquartile range (Q3 - Q1) |

All functions validate input via `validate_numeric_input()`, which rejects non-numeric, empty, and all-NA vectors with informative error messages.

### How to Run

```r
# Install and test
devtools::install("question_1/descriptive_stats")
devtools::test("question_1/descriptive_stats")
devtools::check("question_1/descriptive_stats")  # 0 errors, 0 warnings
```

### Key Design Decisions

- **Shared validation:** `validate_numeric_input()` centralizes NA removal and type checking, keeping each function focused on its computation.
- **Multimodal support:** `calc_mode()` returns all tied modes as a sorted vector. When all values have equal frequency, it returns `NA` with an informative message rather than an error.
- **Edge cases tested:** `Inf`/`NaN` propagation, integer coercion, `NULL` rejection, two-element vectors, negative values, boundary values, all-identical inputs.

---

## Question 2: SDTM DS Domain

**Objective:** Create an SDTM Disposition (DS) domain from raw clinical trial data using `{sdtm.oak}`.

**Location:** `question_2_sdtm/`

### Files

| File | Description |
|------|-------------|
| `02_create_ds_domain.R` | Main script — produces `ds_final` (850 rows, 12 columns) |
| `metadata/sdtm_ct.csv` | Controlled terminology mapping (C66727, C66728, C66742, etc.) |
| `test_ds_domain.R` | Test suite — 49 tests, 76 assertions |

### Output Variables

`STUDYID`, `DOMAIN`, `USUBJID`, `DSSEQ`, `DSTERM`, `DSDECOD`, `DSCAT`, `VISITNUM`, `VISIT`, `DSDTC`, `DSSTDTC`, `DSSTDY`

### sdtm.oak Functions Used

| Function | Purpose |
|----------|---------|
| `assign_no_ct()` | Map raw fields without controlled terminology |
| `assign_ct()` | Map raw fields with CT lookup |
| `hardcode_ct()` | Assign fixed CT values (e.g., DOMAIN = "DS") |
| `assign_datetime()` | Convert raw dates to ISO 8601 |
| `derive_seq()` | Generate sequential DSSEQ per subject |
| `derive_study_day()` | Calculate DSSTDY relative to RFSTDTC |
| `condition_add()` | Conditionally assign DSCAT based on DSDECOD |
| `ct_map()` | Load controlled terminology from CSV |

### Data Source

`pharmaverseraw::ds_raw` — 850 records, 306 subjects from CDISCPILOT01.

### How to Run

```r
source("question_2_sdtm/02_create_ds_domain.R")
source("question_2_sdtm/test_ds_domain.R")  # 49 tests
```

### Key Design Decisions

- **Three DSCAT categories** derived via `condition_add()`: PROTOCOL MILESTONE (Randomized), DISPOSITION EVENT (other coded events), OTHER EVENT (OTHERSP records with NA DSDECOD).
- **Study day derivation** uses `derive_study_day()` with RFSTDTC from DM; Screen Failure subjects correctly get NA DSSTDY.
- **Edge cases tested:** Screen Failure NA study days, negative DSSTDY (events before reference date), unscheduled/unmapped visits, DEATH records, DSDTC vs DSSTDTC from different raw sources.

---

## Question 3: ADaM ADSL Dataset

**Objective:** Create a Subject Level (ADSL) dataset from SDTM source data using `{admiral}` and tidyverse tools.

**Location:** `question_3_adam/`

### Files

| File | Description |
|------|-------------|
| `create_adsl.R` | Main script — produces `adsl` (306 rows, 31 columns) |
| `test_adsl.R` | Test suite — 75 tests, 99 assertions |

### Derived Variables

| Variable | Derivation | Source |
|----------|-----------|--------|
| `AGEGR9` / `AGEGR9N` | Age grouping: <18 (1), 18-50 (2), >50 (3) | DM.AGE |
| `TRTSDTM` / `TRTSTMF` | Treatment start datetime with time imputation flag "H" | EX.EXSTDTC |
| `ITTFL` | Intent-to-treat flag: "Y" if ARM is populated | DM.ARM |
| `ABNSBPFL` | Abnormal systolic BP flag: "Y" if any SYSBP >= 140 or < 100 mmHg | VS |
| `LSTALVDT` | Last known alive date: max of VS, AE, DS dates | VS, AE, DS |
| `CARPOPFL` | Cardiac population flag: "Y" if any cardiac disorder AE | AE.AESOC |

### admiral Functions Used

`derive_vars_cat()`, `derive_vars_dtm()`, `derive_vars_dt()`, `derive_vars_dtm_to_dt()`, `derive_vars_merged()`, `derive_var_merged_exist_flag()`, `derive_vars_extreme_event()`, `convert_blanks_to_na()`

### R Packages

`admiral`, `pharmaversesdtm`, `dplyr`, `lubridate`, `stringr`, `tidyr`

### How to Run

```r
source("question_3_adam/create_adsl.R")
source("question_3_adam/test_adsl.R")  # 75 tests
```

### Key Design Decisions

- **LSTALVDT** is the maximum date across three domains (VS, AE, DS), using `derive_vars_extreme_event()` with `mode = "last"`.
- **ABNSBPFL** uses `derive_var_merged_exist_flag()` to check for any abnormal SYSBP reading per subject.
- **Screen Failure handling:** 52 subjects with ARMCD="Scrnfail" correctly get NA treatment dates, ABNSBPFL="N", and CARPOPFL=NA.
- **Edge cases tested:** AGE=50 boundary, Screen Failure subjects, death subjects, treatment date consistency (TRTSDT <= TRTEDT), cross-variable consistency.

---

## Question 4: Clinical Reporting (TLGs)

**Objective:** Create Tables, Listings, and Graphs for adverse events using ADAE data.

**Location:** `question_4_tlg/`

### Files

| File | Description |
|------|-------------|
| `01_create_ae_summary_table.R` | Hierarchical AE summary table using `{gtsummary}` |
| `02_create_visualizations.R` | Stacked bar chart + forest plot using `{ggplot2}` |
| `03_create_listings.R` | AE listing using `{gt}` |
| `test_tlg.R` | Test suite — 50 tests, 75 assertions |
| `ae_summary_table.html` | Output: AE summary by SOC/PT and treatment arm |
| `ae_severity_by_treatment.png` | Output: Stacked bar chart of severity by arm |
| `ae_top10_forest_plot.png` | Output: Forest plot with Clopper-Pearson 95% CIs |
| `ae_listings.html` | Output: Detailed AE listing sorted by subject and date |

### Outputs

**1. AE Summary Table** (`01_create_ae_summary_table.R`)
- FDA Table 10 format using `tbl_hierarchical()` from `{gtsummary}`
- Stratified by ACTARM (Placebo, Xanomeline High Dose, Xanomeline Low Dose)
- Hierarchical: AESOC > AEDECOD with unique subject counts

**2. Visualizations** (`02_create_visualizations.R`)
- **Stacked bar chart:** AE severity distribution (MILD/MODERATE/SEVERE) by treatment arm
- **Forest plot:** Top 10 AEs by incidence with Clopper-Pearson 95% confidence intervals

**3. AE Listing** (`03_create_listings.R`)
- Detailed listing of all 1122 TEAEs using `{gt}`
- Columns: USUBJID, ACTARM, AEDECOD, AESEV, AEREL, AESTDTC, AEENDTC
- Sorted by subject and AE start date

### Data Source

`pharmaverseadam::adae` — 1191 records filtered to 1122 TEAEs (TRTEMFL="Y", excluding Screen Failure). Safety population: 254 subjects.

### How to Run

```r
source("question_4_tlg/01_create_ae_summary_table.R")
source("question_4_tlg/02_create_visualizations.R")
source("question_4_tlg/03_create_listings.R")
source("question_4_tlg/test_tlg.R")  # 50 tests
```

### Key Design Decisions

- **Clopper-Pearson CIs** (exact binomial) used for the forest plot — appropriate for small-sample proportions in clinical trials.
- **Denominator:** 217 unique subjects with TEAEs (not 254 safety population) for incidence calculation.
- **Edge cases tested:** CI boundary properties for n=1 and n=N, ongoing AEs (NA AEENDTC) across all arms, AEREL NA values, severity distribution sums.

---

## Question 5: Clinical Data API (FastAPI)

**Objective:** Build a RESTful API serving clinical trial AE data with cohort filtering and risk scoring.

**Location:** `question_5_api/`

### Files

| File | Description |
|------|-------------|
| `app.py` | FastAPI application with 3 endpoints |
| `test_app.py` | Test suite — 38 tests (pytest) |
| `adae.csv` | AE dataset (1191 records, 225 subjects) |
| `requirements.txt` | Python dependencies |

### API Endpoints

| Method | Path | Description |
|--------|------|-------------|
| `GET` | `/` | Health check |
| `POST` | `/ae-query` | Filter AEs by severity and/or treatment arm |
| `GET` | `/subject-risk/{subject_id}` | Per-subject weighted risk score |

### POST `/ae-query`

Request body (all fields optional):
```json
{
  "severity": ["MILD", "SEVERE"],
  "treatment_arm": "Placebo"
}
```

Response:
```json
{
  "count": 8,
  "subject_ids": ["01-701-1015", "01-703-1175"]
}
```

### GET `/subject-risk/{subject_id}`

Risk score = sum of severity weights: MILD=1, MODERATE=3, SEVERE=5.

| Score | Category |
|-------|----------|
| 0-4 | Low |
| 5-14 | Medium |
| 15+ | High |

Response:
```json
{
  "subject_id": "01-701-1015",
  "risk_score": 3,
  "risk_category": "Low",
  "ae_count": 3
}
```

### How to Run

```bash
cd question_5_api
python -m venv .venv && . .venv/bin/activate
pip install -r requirements.txt
uvicorn app:app --reload          # Start server
pytest test_app.py -v             # Run 38 tests
```

### Key Design Decisions

- **Pydantic models** for request/response validation with type hints.
- **Case-insensitive** severity filtering (input uppercased before matching).
- **Sorted subject IDs** in all responses for deterministic output.
- **Edge cases tested:** case insensitivity, combined filters (SEVERE+Placebo), boundary risk scores (0, 5, 15), empty series, unknown severity values, invalid JSON returns 422.

---

## Question 6: GenAI Clinical Data Assistant

**Objective:** Translate natural language queries about adverse events into structured Pandas filters using an LLM.

**Location:** `question_6_genai/`

### Files

| File | Description |
|------|-------------|
| `clinical_data_agent.py` | Agent with MockLLM and OpenAI LLM support |
| `test_agent.py` | Demo script (runs 3 example queries) |
| `test_clinical_data_agent.py` | Test suite — 70 tests (pytest) |
| `adae.csv` | AE dataset (1191 records, 35 columns) |
| `requirements.txt` | Python dependencies |

### Architecture

```
User Query (natural language)
    │
    ▼
MockLLM.parse() / OpenAI LLM
    │  Keyword matching or LLM prompt
    ▼
QueryIntent(target_column, filter_value)
    │  Pydantic model
    ▼
ClinicalTrialDataAgent.execute_filter()
    │  Pandas: exact match (AESEV, AESER, AEREL)
    │          or partial match (AETERM, AESOC)
    ▼
QueryResult(query, target_column, filter_value, n_subjects, subject_ids)
```

### Supported Query Types

| Category | Example Query | Maps To |
|----------|--------------|---------|
| Severity | "Show severe AEs" | AESEV = SEVERE |
| Seriousness | "Serious adverse events" | AESER = Y |
| Causality | "Probably related events" | AEREL = PROBABLE |
| Body system | "Cardiac disorders" | AESOC = CARDIAC DISORDERS |
| Specific AE | "Subjects with headache" | AETERM contains HEADACHE |
| Fallback | "Show hypotension cases" | AETERM contains HYPOTENSION |

The MockLLM supports 12 body system mappings, 10+ specific AE terms, and falls back to AETERM search with stop-word removal for unrecognized terms.

### How to Run

```bash
cd question_6_genai
python -m venv .venv && . .venv/bin/activate
pip install -r requirements.txt

# With OpenAI (optional)
export OPENAI_API_KEY="sk-..."
python test_agent.py

# Without API key (uses MockLLM)
python test_agent.py
pytest test_clinical_data_agent.py -v  # 70 tests
```

### Key Design Decisions

- **Dual LLM support:** `MockLLM` for deterministic testing without API keys; `OpenAI` via LangChain for production use.
- **Exact vs partial matching:** AESEV/AESER/AEREL use exact equality; AETERM/AESOC use `str.contains()` for partial matching (e.g., "PRURITUS" matches both "PRURITUS" and "APPLICATION SITE PRURITUS").
- **First-match keyword priority:** When a query contains multiple keywords (e.g., "severe cardiac"), the first matched keyword determines the filter.
- **Edge cases tested:** empty results, unknown terms, case insensitivity, partial matching, Pydantic validation, QueryResult truncation at 10 IDs, exact vs partial filter behavior.

---

## Prerequisites

### R (Questions 1-4)

- R >= 4.2.0
- Packages: `admiral`, `sdtm.oak`, `pharmaversesdtm`, `pharmaverseadam`, `pharmaverseraw`, `gtsummary`, `gt`, `ggplot2`, `dplyr`, `tidyr`, `lubridate`, `stringr`, `testthat`, `devtools`

### Python (Questions 5-6)

- Python >= 3.10
- Q5: `fastapi`, `uvicorn`, `pandas`, `httpx`, `pytest`
- Q6: `pandas`, `langchain`, `langchain-openai`, `pydantic`, `pytest`

Each Python question has its own virtual environment (`.venv/`) and `requirements.txt`.

## Running All Tests

```bash
# Q1: R package tests (69 assertions)
cd question_1/descriptive_stats && Rscript -e 'devtools::test()'

# Q2: SDTM tests (49 tests)
Rscript question_2_sdtm/test_ds_domain.R

# Q3: ADaM tests (75 tests)
Rscript question_3_adam/test_adsl.R

# Q4: TLG tests (50 tests)
Rscript question_4_tlg/test_tlg.R

# Q5: API tests (38 tests)
cd question_5_api && . .venv/bin/activate && pytest test_app.py -v

# Q6: GenAI tests (70 tests)
cd question_6_genai && . .venv/bin/activate && pytest test_clinical_data_agent.py -v
```
